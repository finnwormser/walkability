{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c92a128-c816-40b7-9948-6eef22798662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded block_groups from cache.\n",
      "Connecting on dm-mongovm-001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_442992/4283459366.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['latitude'] = df['geo'].apply(lambda g: g.get('coordinates', [None, None])[1])\n",
      "/tmp/ipykernel_442992/4283459366.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['longitude'] = df['geo'].apply(lambda g: g.get('coordinates', [None, None])[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        GEOID10  \\\n",
      "0  130639800001   \n",
      "1  420370513001   \n",
      "2  481130174001   \n",
      "3  510594809021   \n",
      "4  510594822034   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            aggregated_counters  \n",
      "0                                                                                                                                                                                                                                                                                                                                                               {'i'm': {'count': 1, 'count_no_rt': 1}, 'at': {'count': 1, 'count_no_rt': 1}, 'hartsfield-jackson': {'count': 1, 'count_no_rt': 1}, 'atlanta': {'count': 2, 'count_no_rt': 2}, 'international': {'count': 1, 'count_no_rt': 1}, 'airport': {'count': 1, 'count_no_rt': 1}, '-': {'count': 1, 'count_no_rt': 1}, '@atlanta_airport': {'count': 1, 'count_no_rt': 1}, 'in': {'count': 1, 'count_no_rt': 1}, ',': {'count': 1, 'count_no_rt': 1}, 'ga': {'count': 1, 'count_no_rt': 1}, 'https://t.co/eirlbbrwno': {'count': 1, 'count_no_rt': 1}}  \n",
      "1                                                        {'this': {'count': 1, 'count_no_rt': 1}, '#retail': {'count': 1, 'count_no_rt': 1}, '#job': {'count': 1, 'count_no_rt': 1}, 'might': {'count': 1, 'count_no_rt': 1}, 'be': {'count': 1, 'count_no_rt': 1}, 'a': {'count': 1, 'count_no_rt': 1}, 'great': {'count': 1, 'count_no_rt': 1}, 'fit': {'count': 1, 'count_no_rt': 1}, 'for': {'count': 1, 'count_no_rt': 1}, 'you': {'count': 1, 'count_no_rt': 1}, ':': {'count': 1, 'count_no_rt': 1}, 'retail': {'count': 1, 'count_no_rt': 1}, 'store': {'count': 1, 'count_no_rt': 1}, 'positions': {'count': 1, 'count_no_rt': 1}, '-': {'count': 1, 'count_no_rt': 1}, 'https://t.co/ezirtxbniy': {'count': 1, 'count_no_rt': 1}, '#catawissa': {'count': 1, 'count_no_rt': 1}, ',': {'count': 1, 'count_no_rt': 1}, 'pa': {'count': 1, 'count_no_rt': 1}, '#hiring': {'count': 1, 'count_no_rt': 1}}  \n",
      "2                                      {'we're': {'count': 1, 'count_no_rt': 1}, '#hiring': {'count': 1, 'count_no_rt': 1}, '!': {'count': 1, 'count_no_rt': 1}, 'read': {'count': 1, 'count_no_rt': 1}, 'about': {'count': 1, 'count_no_rt': 1}, 'our': {'count': 1, 'count_no_rt': 1}, 'latest': {'count': 1, 'count_no_rt': 1}, '#job': {'count': 1, 'count_no_rt': 1}, 'opening': {'count': 1, 'count_no_rt': 1}, 'here': {'count': 1, 'count_no_rt': 1}, ':': {'count': 1, 'count_no_rt': 1}, 'service': {'count': 1, 'count_no_rt': 1}, 'assistant': {'count': 1, 'count_no_rt': 1}, '-': {'count': 1, 'count_no_rt': 1}, 'https://t.co/ibtklbozu9': {'count': 1, 'count_no_rt': 1}, '#ihop': {'count': 1, 'count_no_rt': 1}, '#mesquite': {'count': 1, 'count_no_rt': 1}, ',': {'count': 1, 'count_no_rt': 1}, 'tx': {'count': 1, 'count_no_rt': 1}, '#customerservice': {'count': 1, 'count_no_rt': 1}}  \n",
      "3                                                                                                                                             {'see': {'count': 1, 'count_no_rt': 1}, 'our': {'count': 1, 'count_no_rt': 1}, 'latest': {'count': 1, 'count_no_rt': 1}, '#herndon': {'count': 1, 'count_no_rt': 1}, ',': {'count': 1, 'count_no_rt': 1}, 'va': {'count': 1, 'count_no_rt': 1}, '#job': {'count': 1, 'count_no_rt': 1}, 'and': {'count': 1, 'count_no_rt': 1}, 'click': {'count': 1, 'count_no_rt': 1}, 'to': {'count': 1, 'count_no_rt': 1}, 'apply': {'count': 1, 'count_no_rt': 1}, ':': {'count': 1, 'count_no_rt': 1}, 'pt': {'count': 1, 'count_no_rt': 1}, 'admin': {'count': 1, 'count_no_rt': 1}, '-': {'count': 1, 'count_no_rt': 1}, 'https://t.co/adert9tzfk': {'count': 1, 'count_no_rt': 1}, '#banking': {'count': 1, 'count_no_rt': 1}, '#hiring': {'count': 1, 'count_no_rt': 1}}  \n",
      "4  {'this': {'count': 1, 'count_no_rt': 1}, '#businessmgmt': {'count': 1, 'count_no_rt': 1}, '#job': {'count': 1, 'count_no_rt': 1}, 'might': {'count': 1, 'count_no_rt': 1}, 'be': {'count': 1, 'count_no_rt': 1}, 'a': {'count': 1, 'count_no_rt': 1}, 'great': {'count': 1, 'count_no_rt': 1}, 'fit': {'count': 1, 'count_no_rt': 1}, 'for': {'count': 1, 'count_no_rt': 1}, 'you': {'count': 1, 'count_no_rt': 1}, ':': {'count': 1, 'count_no_rt': 1}, 'consulting': {'count': 1, 'count_no_rt': 1}, 'practice': {'count': 1, 'count_no_rt': 1}, 'manager': {'count': 1, 'count_no_rt': 1}, '-': {'count': 1, 'count_no_rt': 1}, 'https://t.co/qvsqexqen1': {'count': 1, 'count_no_rt': 1}, '#reston': {'count': 1, 'count_no_rt': 1}, ',': {'count': 1, 'count_no_rt': 1}, 'va': {'count': 1, 'count_no_rt': 1}, '#hiring': {'count': 1, 'count_no_rt': 1}, '#careerarc': {'count': 1, 'count_no_rt': 1}}  \n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from data_mountain_query.counters import AmbientTweetCounters, Counters\n",
    "from data_mountain_query.sentiment import load_happs_scores, df_sentiment\n",
    "from data_mountain_query.counters import lang_dict\n",
    "from data_mountain_query.connection import get_connection\n",
    "from data_mountain_query.query import get_tweets\n",
    "from data_mountain_query.parsers import load_ngrams_parser, parse_ngrams_tweet\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "import fiona\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Import your project-specific modules as needed.\n",
    "# ---------------------------------------------------\n",
    "# from data_mountain_query.connection import get_connection\n",
    "# from data_mountain_query.query import get_tweets\n",
    "# from data_mountain_query.sentiment import load_happs_scores, df_sentiment\n",
    "# from data_mountain_query.parsers import load_ngrams_parser, parse_ngrams_tweet\n",
    "# from data_mountain_query.counters import lang_dict\n",
    "\n",
    "gdb_path = \"/gpfs2/scratch/pwormser/research/SmartLocationDatabase_copy.gdb\"\n",
    "cache_file = \"block_groups.pkl\"\n",
    "\n",
    "if os.path.exists(cache_file):\n",
    "    block_groups = gpd.GeoDataFrame(pd.read_pickle(cache_file))\n",
    "    print(\"Loaded block_groups from cache.\")\n",
    "else:\n",
    "    # List available layers if needed:\n",
    "    import fiona\n",
    "    layers = fiona.listlayers(gdb_path)\n",
    "    print(\"Available layers:\", layers)\n",
    "    \n",
    "    block_groups = gpd.read_file(gdb_path, layer=\"EPA_SLD_Database_V3\")\n",
    "    # Optionally reproject if needed:\n",
    "    block_groups = block_groups.to_crs(\"EPSG:4326\")\n",
    "    # Cache the result for future runs:\n",
    "    block_groups.to_pickle(cache_file)\n",
    "    print(\"Read block_groups from GDB and cached it.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "\n",
    "def extract_lat_lon(df):\n",
    "    # Instead of making a full copy, we use .loc assignment directly.\n",
    "    df['latitude'] = df['geo'].apply(lambda g: g.get('coordinates', [None, None])[1])\n",
    "    df['longitude'] = df['geo'].apply(lambda g: g.get('coordinates', [None, None])[0])\n",
    "    return df  # Now df is modified in-place.\n",
    "\n",
    "def filter_us_tweets(df):\n",
    "    # In-place boolean indexing returns a copy already.\n",
    "    continental = ((df['latitude'] >= 24.396308) & (df['latitude'] <= 49.384358) &\n",
    "                   (df['longitude'] >= -125.0) & (df['longitude'] <= -66.93457))\n",
    "    alaska = ((df['latitude'] >= 51.2) & (df['latitude'] <= 71.4) &\n",
    "              (df['longitude'] >= -179.1) & (df['longitude'] <= -129.9))\n",
    "    hawaii = ((df['latitude'] >= 18.9) & (df['latitude'] <= 22.2) &\n",
    "              (df['longitude'] >= -160.3) & (df['longitude'] <= -154.8))\n",
    "    us_mask = continental | alaska | hawaii\n",
    "    return df[us_mask]  # This creates a new DataFrame (which is unavoidable)\n",
    "\n",
    "def refine_with_us_boundary(df):\n",
    "    # Build GeoDataFrame without extra copying if possible.\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(df['longitude'], df['latitude'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    # Reproject in place (returns a new GeoDataFrame, but we assign it immediately)\n",
    "    gdf = gdf.to_crs(block_groups.crs)\n",
    "    us_boundary_reproj = us_boundary.to_crs(gdf.crs)\n",
    "    joined = gpd.sjoin(gdf, us_boundary_reproj, how=\"inner\", predicate=\"within\")\n",
    "    joined = joined.reset_index(drop=True)\n",
    "    # Only drop geometry and the added index_right\n",
    "    return joined.drop(columns=[\"geometry\", \"index_right\"], errors='ignore')\n",
    "\n",
    "def tag_tweets_with_geodata(df):\n",
    "    target_crs = block_groups.crs\n",
    "    zip_shp = '/gpfs2/scratch/pwormser/research/USA_Boundaries_2022_-232574676275878974/USA_ZipCode.shp'\n",
    "    zip_codes_local = gpd.read_file(zip_shp).to_crs(target_crs)\n",
    "    if 'index_right' in zip_codes_local.columns:\n",
    "        zip_codes_local.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "    bg = block_groups  # using global block_groups\n",
    "    if 'index_right' in bg.columns:\n",
    "        bg = bg.drop(columns=['index_right'])\n",
    "    \n",
    "    # Create GeoDataFrame from df without extra copy if possible.\n",
    "    points_gdf = gpd.GeoDataFrame(df, geometry=[Point(xy) for xy in zip(df['longitude'], df['latitude'])], crs=\"EPSG:4326\")\n",
    "    points_gdf = points_gdf.to_crs(target_crs)\n",
    "    if 'index_right' in points_gdf.columns:\n",
    "        points_gdf.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "    # Spatial join with block groups.\n",
    "    points_bg = gpd.sjoin(points_gdf, bg[['GEOID10', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "    if 'index_right' in points_bg.columns:\n",
    "        points_bg.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "    # Spatial join with ZIP codes.\n",
    "    points_zip = gpd.sjoin(points_gdf, zip_codes_local[['ZIP_CODE', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "    if 'index_right' in points_zip.columns:\n",
    "        points_zip.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "    # Assign new columns into df without copying the whole DataFrame.\n",
    "    df['GEOID10'] = points_bg['GEOID10'].values\n",
    "    df['ZIP_Code'] = points_zip['ZIP_CODE'].values\n",
    "    return df\n",
    "\n",
    "def merge_counters(counter_list):\n",
    "    aggregated = {}\n",
    "    for counter in counter_list:\n",
    "        for word, counts in counter.items():\n",
    "            if word not in aggregated:\n",
    "                aggregated[word] = counts.copy()\n",
    "            else:\n",
    "                for key, value in counts.items():\n",
    "                    aggregated[word][key] = aggregated[word].get(key, 0) + value\n",
    "    return aggregated\n",
    "\n",
    "def aggregate_counters_by_block(df):\n",
    "    df['counters'] = df['counters'].apply(lambda x: x if isinstance(x, dict) else {})\n",
    "    grouped = df.groupby('GEOID10', as_index=False)['counters'].agg(\n",
    "        lambda counters: merge_counters(list(counters))\n",
    "    )\n",
    "    grouped.columns = ['GEOID10', 'aggregated_counters']\n",
    "    return grouped\n",
    "\n",
    "def process_chunk(chunk_df):\n",
    "    chunk_df = extract_lat_lon(chunk_df)\n",
    "    chunk_df = filter_us_tweets(chunk_df)\n",
    "    chunk_df = refine_with_us_boundary(chunk_df)\n",
    "    chunk_df = tag_tweets_with_geodata(chunk_df)\n",
    "    agg_df = aggregate_counters_by_block(chunk_df)\n",
    "    # Force agg_df to be a DataFrame by resetting the index if necessary:\n",
    "    if isinstance(agg_df, pd.Series):\n",
    "        agg_df = agg_df.reset_index()\n",
    "    return {row['GEOID10']: row['aggregated_counters'] for _, row in agg_df.iterrows()}\n",
    "\n",
    "# ----------------------------\n",
    "# Main Processing Pipeline\n",
    "# ----------------------------\n",
    "if __name__ == '__main__':\n",
    "    start_date = datetime(2016, 5, 1)\n",
    "    end_date = datetime(2016, 5, 2)\n",
    "    lang = 'en'\n",
    "    \n",
    "    collection, client = get_connection(geotweets=True)\n",
    "    query = {'tweet_created_at': {'$gte': start_date, '$lt': end_date}, \"fastText_lang\": \"en\"}\n",
    "    \n",
    "    tweets = get_tweets(collection, query, limit=25)\n",
    "    \n",
    "    # Add the \"counters\" key using the n-gram parser.\n",
    "    ngrams_parser = load_ngrams_parser()\n",
    "    tweets = [{**tweet, \"counters\": parse_ngrams_tweet(tweet, ngrams_parser)} for tweet in tweets]\n",
    "    df_all = pd.DataFrame(list(tweets))\n",
    "    \n",
    "    global us_boundary\n",
    "    us_boundary = gpd.read_file('/gpfs2/scratch/pwormser/research/cb_2018_us_nation_20m/cb_2018_us_nation_20m.shp')\n",
    "    us_boundary = us_boundary.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    word2score = load_happs_scores(lang=lang_dict[lang])\n",
    "    df_all = df_sentiment(df_all, word2score=word2score)\n",
    "    \n",
    "    # Process tweets in chunks\n",
    "    chunk_size = 1000  # For testing; use 1000+ for production.\n",
    "    all_agg = {}\n",
    "    for i in range(0, len(df_all), chunk_size):\n",
    "        chunk_df = df_all.iloc[i : i + chunk_size]\n",
    "        chunk_result = process_chunk(chunk_df)\n",
    "        for geoid, counters in chunk_result.items():\n",
    "            if geoid in all_agg:\n",
    "                all_agg[geoid] = merge_counters([all_agg[geoid], counters])\n",
    "            else:\n",
    "                all_agg[geoid] = counters\n",
    "    \n",
    "    final_agg = pd.DataFrame({\n",
    "        'GEOID10': list(all_agg.keys()),\n",
    "        'aggregated_counters': list(all_agg.values())\n",
    "    })\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(final_agg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69df4a-6fe3-4dc4-994e-5d2f04961cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
