{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c92a128-c816-40b7-9948-6eef22798662",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata_mountain_query\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_ngrams_parser, parse_ngrams_tweet\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Point\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from data_mountain_query.counters import AmbientTweetCounters, Counters\n",
    "from data_mountain_query.sentiment import load_happs_scores, df_sentiment\n",
    "from data_mountain_query.counters import lang_dict\n",
    "from data_mountain_query.connection import get_connection\n",
    "from data_mountain_query.query import get_tweets\n",
    "from data_mountain_query.parsers import load_ngrams_parser, parse_ngrams_tweet\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "import fiona\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "gdb_path = \"/gpfs2/scratch/pwormser/research/SmartLocationDatabase_copy.gdb\"\n",
    "cache_file = \"block_groups.pkl\"\n",
    "\n",
    "if os.path.exists(cache_file):\n",
    "    block_groups = gpd.GeoDataFrame(pd.read_pickle(cache_file))\n",
    "    print(\"Loaded block_groups from cache.\")\n",
    "else:\n",
    "    import fiona\n",
    "    layers = fiona.listlayers(gdb_path)\n",
    "    print(\"Available layers:\", layers)\n",
    "    \n",
    "    block_groups = gpd.read_file(gdb_path, layer=\"EPA_SLD_Database_V3\")\n",
    "    # Optionally reproject if needed:\n",
    "    block_groups = block_groups.to_crs(\"EPSG:4326\")\n",
    "    # Cache the result for future runs:\n",
    "    block_groups.to_pickle(cache_file)\n",
    "    print(\"Read block_groups from GDB and cached it.\")\n",
    "\n",
    "\n",
    "# Utility Functions\n",
    "def extract_lat_lon(df):\n",
    "    \"\"\"\n",
    "    Extracts latitude and longitude from the 'geo' column of the DataFrame \n",
    "    and adds them as new columns.\n",
    "    \"\"\"\n",
    "    print(df)\n",
    "    df['latitude'] = df['geo'].apply(lambda g: g.get('coordinates', [None, None])[1])\n",
    "    df['longitude'] = df['geo'].apply(lambda g: g.get('coordinates', [None, None])[0])\n",
    "    print(df)\n",
    "    return df  # Now df is modified in-place.\n",
    "\n",
    "def filter_us_tweets(df):\n",
    "    \"\"\"\n",
    "    Filters tweets to include only those from the continental US, Alaska, and Hawaii\n",
    "    based on latitude and longitude boundaries. Just a quick intitial grouping to get\n",
    "    rid of most global tweets quickly to save memory.\n",
    "    \"\"\"\n",
    "    continental = ((df['latitude'] >= 24.396308) & (df['latitude'] <= 49.384358) &\n",
    "                   (df['longitude'] >= -125.0) & (df['longitude'] <= -66.93457))\n",
    "    alaska = ((df['latitude'] >= 51.2) & (df['latitude'] <= 71.4) &\n",
    "              (df['longitude'] >= -179.1) & (df['longitude'] <= -129.9))\n",
    "    hawaii = ((df['latitude'] >= 18.9) & (df['latitude'] <= 22.2) &\n",
    "              (df['longitude'] >= -160.3) & (df['longitude'] <= -154.8))\n",
    "    us_mask = continental | alaska | hawaii\n",
    "    return df[us_mask]  \n",
    "\n",
    "def refine_with_us_boundary(df):\n",
    "    \"\"\"\n",
    "    Refines the dataset by ensuring that tweets fall within the official US boundary \n",
    "    using a spatial join with predefined boundary data.\n",
    "    \"\"\"\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(df['longitude'], df['latitude'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    gdf = gdf.to_crs(block_groups.crs)\n",
    "    us_boundary_reproj = us_boundary.to_crs(gdf.crs)\n",
    "    joined = gpd.sjoin(gdf, us_boundary_reproj, how=\"inner\", predicate=\"within\")\n",
    "    joined = joined.reset_index(drop=True)\n",
    "    # Only drop geometry and the added index_right\n",
    "    return joined.drop(columns=[\"geometry\", \"index_right\"], errors='ignore')\n",
    "\n",
    "def tag_tweets_with_geodata(df):\n",
    "    \"\"\"\n",
    "    Assigns geographical identifiers (GEOID10 and ZIP_Code) to tweets by \n",
    "    performing spatial joins with block group and ZIP code boundaries.\n",
    "    \"\"\"\n",
    "    target_crs = block_groups.crs\n",
    "    zip_shp = '/gpfs2/scratch/pwormser/research/USA_Boundaries_2022_-232574676275878974/USA_ZipCode.shp'\n",
    "    zip_codes_local = gpd.read_file(zip_shp).to_crs(target_crs)\n",
    "    if 'index_right' in zip_codes_local.columns:\n",
    "        zip_codes_local.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "    bg = block_groups  # using global block_groups\n",
    "    if 'index_right' in bg.columns:\n",
    "        bg = bg.drop(columns=['index_right'])\n",
    "    \n",
    "    points_gdf = gpd.GeoDataFrame(df, geometry=[Point(xy) for xy in zip(df['longitude'], df['latitude'])], crs=\"EPSG:4326\")\n",
    "    points_gdf = points_gdf.to_crs(target_crs)\n",
    "    if 'index_right' in points_gdf.columns:\n",
    "        points_gdf.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "    # Spatial join with block groups.\n",
    "    points_bg = gpd.sjoin(points_gdf, bg[['GEOID10', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "    if 'index_right' in points_bg.columns:\n",
    "        points_bg.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "    # Spatial join with ZIP codes.\n",
    "    points_zip = gpd.sjoin(points_gdf, zip_codes_local[['ZIP_CODE', 'geometry']], how=\"left\", predicate=\"within\")\n",
    "    if 'index_right' in points_zip.columns:\n",
    "        points_zip.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "    df['GEOID10'] = points_bg['GEOID10'].values\n",
    "    df['ZIP_Code'] = points_zip['ZIP_CODE'].values\n",
    "    return df\n",
    "\n",
    "def merge_counters(counter_list):\n",
    "    \"\"\"\n",
    "    Merges a list of Counter dictionaries, summing values for each key across \n",
    "    all counters.\n",
    "    \"\"\"\n",
    "    aggregated = {}\n",
    "    for counter in counter_list:\n",
    "        for word, counts in counter.items():\n",
    "            if word not in aggregated:\n",
    "                aggregated[word] = counts.copy()\n",
    "            else:\n",
    "                for key, value in counts.items():\n",
    "                    aggregated[word][key] = aggregated[word].get(key, 0) + value\n",
    "    return aggregated\n",
    "\n",
    "def aggregate_counters_by_block(df):\n",
    "    \"\"\"\n",
    "    Aggregates tweet content counters by GEOID10 (geographical block group).\n",
    "    \"\"\"\n",
    "    df['counters'] = df['counters'].apply(lambda x: x if isinstance(x, dict) else {})\n",
    "    grouped = df.groupby('GEOID10', as_index=False)['counters'].agg(\n",
    "        lambda counters: merge_counters(list(counters))\n",
    "    )\n",
    "    grouped.columns = ['GEOID10', 'aggregated_counters']\n",
    "    return grouped\n",
    "\n",
    "def process_chunk(chunk_df):\n",
    "    \"\"\"\n",
    "    Processes a chunk of tweet data by extracting location data, filtering for \n",
    "    US tweets, refining location within US boundaries, tagging with geospatial data, \n",
    "    and aggregating tweet content by block group.\n",
    "    \"\"\"\n",
    "    chunk_df = extract_lat_lon(chunk_df)\n",
    "    chunk_df = filter_us_tweets(chunk_df)\n",
    "    chunk_df = refine_with_us_boundary(chunk_df)\n",
    "    chunk_df = tag_tweets_with_geodata(chunk_df)\n",
    "    agg_df = aggregate_counters_by_block(chunk_df)\n",
    "    # Force agg_df to be a DataFrame by resetting the index if necessary:\n",
    "    if isinstance(agg_df, pd.Series):\n",
    "        agg_df = agg_df.reset_index()\n",
    "    return {row['GEOID10']: row['aggregated_counters'] for _, row in agg_df.iterrows()}\n",
    "\n",
    "\n",
    "\n",
    "# Main Processing Pipeline\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_date = datetime(2016, 5, 1)\n",
    "    end_date = datetime(2016, 5, 2)\n",
    "    lang = 'en'\n",
    "    \n",
    "    collection, client = get_connection(geotweets=True)\n",
    "    query = {'tweet_created_at': {'$gte': start_date, '$lt': end_date}, \"fastText_lang\": \"en\"}\n",
    "    \n",
    "    tweets = get_tweets(collection, query, limit=25)\n",
    "    \n",
    "    # Add the \"counters\" key using the n-gram parser.\n",
    "    ngrams_parser = load_ngrams_parser()\n",
    "    tweets = [{**tweet, \"counters\": parse_ngrams_tweet(tweet, ngrams_parser)} for tweet in tweets]\n",
    "    df_all = pd.DataFrame(list(tweets))\n",
    "    \n",
    "    global us_boundary\n",
    "    us_boundary = gpd.read_file('/gpfs2/scratch/pwormser/research/cb_2018_us_nation_20m/cb_2018_us_nation_20m.shp')\n",
    "    us_boundary = us_boundary.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Process tweets in chunks, change chunk size as needed\n",
    "    chunk_size = 1000 \n",
    "    all_agg = {}\n",
    "    for i in range(0, len(df_all), chunk_size):\n",
    "        chunk_df = df_all.iloc[i : i + chunk_size]\n",
    "        chunk_result = process_chunk(chunk_df)\n",
    "        for geoid, counters in chunk_result.items():\n",
    "            if geoid in all_agg:\n",
    "                all_agg[geoid] = merge_counters([all_agg[geoid], counters])\n",
    "            else:\n",
    "                all_agg[geoid] = counters\n",
    "    \n",
    "    final_agg = pd.DataFrame({\n",
    "        'GEOID10': list(all_agg.keys()),\n",
    "        'aggregated_counters': list(all_agg.values())\n",
    "    })\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(final_agg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
